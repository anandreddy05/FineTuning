{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb99b20f",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "\n",
    "## Low-Rank Adaptation\n",
    "\n",
    "Fine-tuning large language models is a resource intensive process. LoRA is a technique that allows us to fine-tune large language models with a small number of parameters. It works by adding and optimizing smaller matrices to the attention weights, typically reducing trainable parameters by about 90%.\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a `parameter-efficient fine-tuning technique` that `freezes` the pre-trained model weights and injects trainable `rank decomposition` matrices into the model’s layers.\n",
    "\n",
    "Instead of training all model parameters during fine-tuning, LoRA decomposes the weight updates into smaller matrices through low-rank decomposition, significantly reducing the number of trainable parameters while maintaining model performance.\n",
    "\n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e26bd5",
   "metadata": {},
   "source": [
    "# Parameter-efficient fine-tuning (PEFT)\n",
    "\n",
    "PEFT is a library that provides a unified interface for loading and managing PEFT methods, including LoRA. \n",
    "\n",
    "It allows you to easily load and switch between different PEFT methods, making it easier to experiment with different fine-tuning techniques.\n",
    "\n",
    "Adapters can be loaded onto a pretrained model with load_adapter(), which is useful for trying out different adapters whose weights aren’t merged. Set the active adapter weights with the set_adapter() function. To return the base model, you could use unload() to unload all of the LoRA modules. This makes it easy to switch between different task-specific weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e818fc",
   "metadata": {},
   "source": [
    "We’ll use the LoRAConfig class. The setup requires just a few configuration steps:\n",
    "\n",
    "- Define the LoRA configuration (rank, alpha, dropout)\n",
    "- Create the SFTTrainer with PEFT config\n",
    "- Train and save the adapter weights\n",
    "\n",
    "| Parameter       | Description |\n",
    "|----------------|-------------|\n",
    "| **r (rank)**         | Dimension of the low-rank matrices used for weight updates. Typically between 4–32. Lower values provide more compression but potentially less expressiveness. |\n",
    "| **lora_alpha**       | Scaling factor for LoRA layers, usually set to 2× the rank value. Higher values result in stronger adaptation effects. |\n",
    "| **lora_dropout**     | Dropout probability for LoRA layers, typically 0.05–0.1. Higher values help prevent overfitting during training. |\n",
    "| **bias**             | Controls training of bias terms. Options are “none”, “all”, or “lora_only”. “none” is most common for memory efficiency. |\n",
    "| **target_modules**   | Specifies which model modules to apply LoRA to. Can be “all-linear” or specific modules like “q_proj,v_proj”. More modules enable greater adaptability but increase memory usage. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35178553",
   "metadata": {},
   "source": [
    "When implementing PEFT methods, start with small rank values (4-8) for LoRA and monitor training loss. Use validation sets to prevent overfitting and compare results with full fine-tuning baselines when possible. The effectiveness of different methods can vary by task, so experimentation is key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8405491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca822eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9105528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = rank_dimension,\n",
    "    lora_alpha = lora_alpha, # 2x the rank\n",
    "    lora_dropout = lora_dropout,\n",
    "    bias = \"none\",\n",
    "    target_modules = \"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    processing_class=tokenizer,    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAIVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
