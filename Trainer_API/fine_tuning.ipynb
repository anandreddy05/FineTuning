{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ad213",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\n",
    "    \"Hey, I am Anand.\",\n",
    "    \"I am learning trainer.\"\n",
    "]\n",
    "batch = tokenizer(sequence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "batch[\"labels\"] = torch.tensor([1,1])\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b68ab",
   "metadata": {},
   "source": [
    "### padding=True\n",
    "- This adds padding tokens so that all sequences in a batch are the same length.\n",
    "- Transformers expect input of fixed size.\n",
    "- If you input multiple sequences (e.g., a list), the tokenizer finds the longest one and pads the others to match.\n",
    "- Padding tokens usually have a special ID like 0.\n",
    "Without padding:\n",
    "\n",
    "[101, 7592, 999, 102]\n",
    "[101, 2054, 2024, 2017, 102]\n",
    "With padding:\n",
    "\n",
    "[101, 7592, 999, 102, 0, 0]\n",
    "[101, 2054, 2024, 2017, 102]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac6912",
   "metadata": {},
   "source": [
    "## truncation=True\n",
    "- This cuts off sequences that are too long for the model’s maximum input length (usually 512 tokens for BERT).\n",
    "\n",
    "- Prevents input from exceeding the model's limits.\n",
    "\n",
    "- Truncation occurs from the end by default (but you can customize).\n",
    "\n",
    "Ex:\n",
    "- \"Hello world\" → OK\n",
    "- \"A very very very very very ... long sentence\" → will be cut off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1326d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136d735",
   "metadata": {},
   "source": [
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbee221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ac9ba",
   "metadata": {},
   "source": [
    "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2feb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1044e4",
   "metadata": {},
   "source": [
    "| Feature           | Padding (`max_length`) | Dynamic Padding (`DataCollatorWithPadding`) |\n",
    "| ----------------- | ---------------------- | ------------------------------------------- |\n",
    "| Padding Length    | Fixed                  | Varies per batch                            |\n",
    "| Memory Efficiency | Low (if varied input)  | High                                        |\n",
    "| Setup             | Simple                 | Slightly more complex                       |\n",
    "| Recommended Use   | Static batches         | Training & inference for variable lengths   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd6ee5",
   "metadata": {},
   "source": [
    "## Fixed Padding (e.g., pad to 8 tokens)\n",
    "Both sequences are padded to 8 tokens, so the attention masks would look like this:\n",
    "Text 1: [Hello, world, PAD, PAD, PAD, PAD, PAD, PAD]\n",
    "Mask  : [  1  ,   1 ,  0 ,  0 ,  0 ,  0 ,  0 ,  0 ]\n",
    "\n",
    "Text 2: [Hugging, Face, makes, great, models, PAD, PAD, PAD]\n",
    "Mask  : [   1  ,  1  ,   1 ,   1 ,    1 ,  0 ,  0 ,  0 ]\n",
    "\n",
    "## Dynamic Padding (pad to longest in batch)\n",
    "If we batch Text 1 and Text 2 together, the longest is 6 tokens, so we pad to 6:\n",
    "Text 1: [Hello, world, PAD, PAD, PAD, PAD]\n",
    "Mask  : [  1  ,   1 ,  0 ,  0 ,  0 ,  0 ]\n",
    "\n",
    "Text 2: [Hugging, Face, makes, great, models, PAD]\n",
    "Mask  : [   1  ,  1  ,   1 ,   1 ,    1 ,  0 ]\n",
    "\n",
    "\n",
    "| Strategy    | Attention Mask Example for Text 1 | Padding Length |\n",
    "| ----------- | --------------------------------- | -------------- |\n",
    "| **Fixed**   | `[1, 1, 0, 0, 0, 0, 0, 0]`        | Always 8       |\n",
    "| **Dynamic** | `[1, 1, 0, 0, 0, 0]`              | Just enough    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768d52d",
   "metadata": {},
   "source": [
    "The function that is responsible for putting together samples inside a batch is called a ```collate function```. It’s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466effc9",
   "metadata": {},
   "source": [
    " Transformers provides a ```Trainer``` class to help you fine-tune any of the pretrained models it provides on your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4a09a",
   "metadata": {},
   "source": [
    "## Training\n",
    "The first step before we can define our Trainer is to define a ```TrainingArguments``` class that will contain all the hyperparameters the Trainer will use for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",                   # Where to save model/checkpoints\n",
    "#     evaluation_strategy=\"epoch\",              # When to evaluate: \"no\", \"steps\", or \"epoch\"\n",
    "#     per_device_train_batch_size=16,           # Batch size for training\n",
    "#     per_device_eval_batch_size=16,            # Batch size for evaluation\n",
    "#     num_train_epochs=3,                       # Total number of training epochs\n",
    "#     weight_decay=0.01,                        # Weight decay for regularization\n",
    "#     logging_dir=\"./logs\",                     # Directory for storing logs\n",
    "#     logging_steps=50,                         # Log every X steps\n",
    "#     save_strategy=\"epoch\",                    # Save checkpoints every epoch\n",
    "#     load_best_model_at_end=True,              # Keep best model according to eval_metric\n",
    "#     metric_for_best_model=\"accuracy\",         # Metric to determine \"best\" model\n",
    "#     save_total_limit=2                        # Keep only the last 2 models\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954899a",
   "metadata": {},
   "source": [
    "| Parameter                     | Purpose                                            | Recommended/Example        |\n",
    "| ----------------------------- | -------------------------------------------------- | -------------------------- |\n",
    "| `output_dir`                  | Directory to save checkpoints and final model      | `\"./results\"`              |\n",
    "| `num_train_epochs`            | How many times the model will see the full dataset | `3` to `5` is common       |\n",
    "| `per_device_train_batch_size` | Training batch size *per GPU*                      | `8`–`32`                   |\n",
    "| `evaluation_strategy`         | When to evaluate (can be `\"epoch\"` or `\"steps\"`)   | `\"epoch\"` is common        |\n",
    "| `save_strategy`               | When to save model (same options as above)         | `\"epoch\"`                  |\n",
    "| `learning_rate`               | Step size for gradient updates                     | Optional: `2e-5` to `5e-5` |\n",
    "| `weight_decay`                | L2 regularization to prevent overfitting           | `0.01`                     |\n",
    "| `logging_dir`                 | For TensorBoard logs                               | `\"./logs\"`                 |\n",
    "| `load_best_model_at_end`      | Automatically restore best checkpoint              | `True`                     |\n",
    "| `metric_for_best_model`       | Metric used for `load_best_model_at_end`           | e.g. `\"accuracy\"`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0428114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    # processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e58c4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a3883",
   "metadata": {},
   "source": [
    "- The output of the ```predict()``` method is another named tuple with three fields: `predictions`, `label_ids`, and `metrics`. \n",
    "- The metrics field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). \n",
    "- Once we complete our `compute_metrics()` function and pass it to the Trainer, that field will also contain the metrics returned by compute_metrics()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12ea45",
   "metadata": {},
   "source": [
    "As you can see, predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.predictions.shape)\n",
    "print(predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aeb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37957f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "metric.compute(predictions=preds,references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520a99a",
   "metadata": {},
   "source": [
    "Logits are the raw, unnormalized predictions that a machine learning model (especially a neural network) outputs before applying an activation function like softmax or sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "    logits,labels = eval_preds\n",
    "    predictions = np.argmax(logits,axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af5717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a886a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAIVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
