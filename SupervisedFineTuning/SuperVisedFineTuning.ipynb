{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00de1074",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6eee5",
   "metadata": {},
   "source": [
    "1️⃣ Chat Templates\n",
    "\n",
    "Chat templates structure interactions between users and AI models, ensuring consistent and contextually appropriate responses. They include components like system prompts and role-based messages.\n",
    "\n",
    "2️⃣ Supervised Fine-Tuning\n",
    "\n",
    "Supervised Fine-Tuning (SFT) is a critical process for adapting pre-trained language models to specific tasks. It involves training the model on a task-specific dataset with labeled examples.\n",
    "\n",
    "3️⃣ Low Rank Adaptation (LoRA)\n",
    "\n",
    "Low Rank Adaptation (LoRA) is a technique for fine-tuning language models by adding low-rank matrices to the model’s layers. This allows for efficient fine-tuning while preserving the model’s pre-trained knowledge. One of the key benefits of LoRA is the significant memory savings it offers, making it possible to fine-tune large models on hardware with limited resources.\n",
    "\n",
    "4️⃣ Evaluation\n",
    "\n",
    "Evaluation is a crucial step in the fine-tuning process. It allows us to measure the performance of the model on a task-specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1768cd0",
   "metadata": {},
   "source": [
    "**Supervised Fine-Tuning (SFT)** is a process primarily used to adapt pre-trained language models to **follow instructions**, engage in dialogue, and use specific output formats.\n",
    "- While pre-trained models have impressive general capabilities, `SFT helps transform them into assistant-like models that can better understand and respond to user prompts`. This is typically done by training on datasets of human-written conversations and instructions.\n",
    "- SFT involves significant computational resources and engineering effort, so it should only be pursued when prompting existing models proves insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f8917",
   "metadata": {},
   "source": [
    "If you determine that SFT is necessary, the decision to proceed depends on two primary factors:\n",
    "\n",
    "- Template Control\n",
    "\n",
    "SFT allows precise control over the model’s output structure. This is particularly valuable when you need the model to:\n",
    "\n",
    "1. Generate responses in a specific chat template format\n",
    "2. Follow strict output schemas\n",
    "3. Maintain consistent styling across responses\n",
    "\n",
    "- Domain Adaptation\n",
    "\n",
    "When working in specialized domains, SFT helps align the model with domain-specific requirements by:\n",
    "\n",
    "1. Teaching domain terminology and concepts\n",
    "2. Enforcing professional standards\n",
    "3. Handling technical queries appropriately\n",
    "4. Following industry-specific guidelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dbbc9d",
   "metadata": {},
   "source": [
    "- Dataset Preparation\n",
    "\n",
    "The supervised fine-tuning process requires a task-specific dataset structured with input-output pairs. Each pair should consist of:\n",
    "\n",
    "1. An input prompt\n",
    "2. The expected model response\n",
    "3. Any additional context or metadata\n",
    "\n",
    "The quality of your training data is crucial for successful fine-tuning. Let’s look at how to prepare and validate your dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e76c4e",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "The success of your fine-tuning depends heavily on choosing the right training parameters. Let’s explore each important parameter and how to configure them effectively:\n",
    "\n",
    "The SFTTrainer configuration requires consideration of several parameters that control the training process. Let’s explore each parameter and their purpose:\n",
    "\n",
    "Training Duration Parameters:\n",
    "\n",
    "`num_train_epochs`: Controls total training duration\n",
    "\n",
    "`max_steps`: Alternative to epochs, sets maximum number of training steps\n",
    "More epochs allow better learning but risk overfitting\n",
    "Batch Size Parameters:\n",
    "\n",
    "`per_device_train_batch_size`: Determines memory usage and training stability\n",
    "\n",
    "`gradient_accumulation_steps`: Enables larger effective batch sizes\n",
    "Larger batches provide more stable gradients but require more memory\n",
    "Learning Rate Parameters:\n",
    "\n",
    "`learning_rate`: Controls size of weight updates\n",
    "\n",
    "`warmup_ratio`: Portion of training used for learning rate warmup\n",
    "Too high can cause instability, too low results in slow learning\n",
    "Monitoring Parameters:\n",
    "\n",
    "`logging_steps`: Frequency of metric logging\n",
    "\n",
    "`eval_steps`: How often to evaluate on validation data\n",
    "\n",
    "`save_steps`: Frequency of model checkpoint saves\n",
    "\n",
    "Start with conservative values and adjust based on monitoring: - Begin with 1-3 epochs - Use smaller batch sizes initially - Monitor validation metrics closely - Adjust learning rate if training is unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f560f",
   "metadata": {},
   "source": [
    "# Transformers Reinforcement Learning (TRL) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c929339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig,SFTTrainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d067e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\",\"all\")\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ea634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import setup_chat_format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer, #handle tokenization automatically\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36508d",
   "metadata": {},
   "source": [
    "When using a dataset with a \"messages\" field (like the example above), the SFTTrainer automatically applies the model's chat template, which it retrieves from the hub. This means you don't need any additional configuration to handle chat-style conversations - the trainer will format the messages according to the model's expected template format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb81e4a",
   "metadata": {},
   "source": [
    "| Feature/Use Case                | `transformers.Trainer`                | `trl.SFTTrainer`                     |\n",
    "| ------------------------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Model Type                      | Any (BERT, T5, GPT, etc.)             | Primarily causal LMs (GPT-like chat) |\n",
    "| Dataset Format                  | Tokenized inputs (input\\_ids, labels) | Raw chat messages (`messages` field) |\n",
    "| Tokenization                    | Manual or external                    | Automatic via `processing_class`     |\n",
    "| Chat/Instruction Formatting     | None                                  | Built-in with chat templates         |\n",
    "| Reinforcement Learning Support  | No                                    | Yes (PPO, DPO, SFT)                  |\n",
    "| Ease for Chat Model Fine-tuning | Requires manual setup                 | Out-of-the-box support               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c4f6d",
   "metadata": {},
   "source": [
    "| Argument                                 | What it controls                     | How to choose                                                                             |\n",
    "| ---------------------------------------- | ------------------------------------ | ----------------------------------------------------------------------------------------- |\n",
    "| **learning\\_rate**                       | Step size for optimizer updates      | Start with typical defaults like 5e-5 or 3e-4; lower for bigger models or sensitive tasks |\n",
    "| **per\\_device\\_train\\_batch\\_size**      | Batch size per GPU/CPU device        | Depends on GPU memory; bigger batch sizes help convergence but require more memory        |\n",
    "| **max\\_steps** or **num\\_train\\_epochs** | Total training duration              | `max_steps` for fixed steps, `num_train_epochs` for full passes over dataset              |\n",
    "| **warmup\\_steps**                        | Steps to gradually increase LR       | Usually 5-10% of total steps; helps stable training start                                 |\n",
    "| **logging\\_steps**                       | How often to log metrics             | Every 10-100 steps, depending on training speed                                           |\n",
    "| **eval\\_steps**                          | How often to evaluate model          | Balance between feedback and overhead; e.g., every 50-100 steps                           |\n",
    "| **save\\_steps**                          | How often to save checkpoints        | Similar to eval steps or longer if saving is slow                                         |\n",
    "| **weight\\_decay**                        | Regularization to reduce overfitting | Typical values 0.01 or 0; adjust if overfitting                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dc6d2",
   "metadata": {},
   "source": [
    "| Feature               | Encoder-Only  | Decoder-Only   | Encoder-Decoder                            |\n",
    "| --------------------- | ------------- | -------------- | ------------------------------------------ |\n",
    "| Direction             | Bidirectional | Unidirectional | Bidirectional (enc) → Unidirectional (dec) |\n",
    "| Pretraining Objective | Masked LM     | Causal LM      | Denoising / Seq2Seq                        |\n",
    "| Text Generation       | ❌ Not suited  | ✅ Excellent    | ✅ Excellent                                |\n",
    "| Classification Tasks  | ✅ Excellent   | ❌ Not ideal    | ✅ With prompt tuning                       |\n",
    "| Output Control        | No generation | Token by token | Token by token                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef98f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAIVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
